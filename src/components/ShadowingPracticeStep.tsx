import React, { useState, useEffect, useRef } from 'react';
import ScriptDisplay from './ScriptDisplay';
import AudioControls from './AudioControls';
import { evaluatePronunciationWithAzure } from '../utils/azureSpeechUtils';

interface ShadowingPracticeStepProps {
  text: string;
  onGoBack: () => void;
  onEvaluate: (audioBlob: Blob) => void;
  onFavorite?: () => void;
  isFavorite?: boolean;
}

interface AudioState {
  isPlaying: boolean;
  isRecording: boolean;
  hasRecording: boolean;
  currentWordIndex: number;
  audioBlob: Blob | null;
}

// ìŒì„± ì˜µì…˜ ì •ì˜
const VOICE_OPTIONS = [
  {
    id: 'zh-CN-YunxiNeural',
    name: 'ì¹œê·¼í•œ ë‚¨ì„±',
    description: 'ë¶€ë“œëŸ½ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‚¨ì„± ëª©ì†Œë¦¬',
    gender: 'male'
  },
  {
    id: 'zh-CN-YunyangNeural',
    name: 'ë‰´ìŠ¤ ì•µì»¤ (ë‚¨ì„±)',
    description: 'ì „ë¬¸ì ì´ê³  ëª…í™•í•œ ë‰´ìŠ¤ ì•µì»¤ ìŠ¤íƒ€ì¼',
    gender: 'male'
  },
  {
    id: 'zh-CN-XiaoxiaoNeural',
    name: 'ì°¨ë¶„í•œ ì—¬ì„±',
    description: 'ë¶€ë“œëŸ½ê³  ì°¨ë¶„í•œ ì—¬ì„± ëª©ì†Œë¦¬',
    gender: 'female'
  },
  {
    id: 'zh-CN-XiaoyiNeural',
    name: 'ë°ì€ ì—¬ì„±',
    description: 'í™œê¸°ì°¨ê³  ì¹œê·¼í•œ ì—¬ì„± ëª©ì†Œë¦¬',
    gender: 'female'
  }
];

const ShadowingPracticeStep: React.FC<ShadowingPracticeStepProps> = ({
  text,
  onGoBack,
  onEvaluate,
  onFavorite,
  isFavorite = false
}) => {
  const [audioState, setAudioState] = useState<AudioState>({
    isPlaying: false,
    isRecording: false,
    hasRecording: false,
    currentWordIndex: -1,
    audioBlob: null
  });

  const [pinyin, setPinyin] = useState<string>('');
  const [isLoadingAudio, setIsLoadingAudio] = useState(false);
  const [selectedVoice, setSelectedVoice] = useState(VOICE_OPTIONS[0]); // ê¸°ë³¸ê°’: ë‰´ìŠ¤ ì•µì»¤
  const [showVoiceSelect, setShowVoiceSelect] = useState(false);
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const synthesizedAudioRef = useRef<HTMLAudioElement | null>(null);
  const highlightTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ ë³‘ìŒ ìƒì„± ë° TTS ìŒì„± ìƒì„±
  useEffect(() => {
    generatePinyin();
    generateTTSAudio();
  }, [text, selectedVoice]);

  // ë³‘ìŒ ìƒì„± (ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ)
  const generatePinyin = () => {
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Azure Translatorë‚˜ ë‹¤ë¥¸ ì¤‘êµ­ì–´ ì²˜ë¦¬ API ì‚¬ìš©
    const mockPinyin = convertToPinyin(text);
    setPinyin(mockPinyin);
  };

  // TTS ìŒì„± ìƒì„±
  const generateTTSAudio = async () => {
    setIsLoadingAudio(true);
    try {
      // Azure Speech Services API í˜¸ì¶œ
      const audioUrl = await generateAzureTTS(text, selectedVoice.id);
      synthesizedAudioRef.current = new Audio(audioUrl);
      
      // ì˜¤ë””ì˜¤ ë¡œë“œ ì™„ë£Œ í›„ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì¶”ê°€
      synthesizedAudioRef.current.addEventListener('loadeddata', () => {
        console.log('Azure TTS ì˜¤ë””ì˜¤ ë¡œë“œ ì™„ë£Œ');
      });
      
      // ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ ì‹œ í•˜ì´ë¼ì´íŠ¸ ë¦¬ì…‹
      synthesizedAudioRef.current.addEventListener('ended', () => {
        setAudioState(prev => ({ ...prev, isPlaying: false, currentWordIndex: -1 }));
      });

      // ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬
      synthesizedAudioRef.current.addEventListener('error', (e) => {
        console.error('Azure TTS ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨:', e);
        alert('Azure ìŒì„± ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      });
    } catch (error) {
      console.error('Azure TTS ìƒì„± ì‹¤íŒ¨:', error);
      alert('Azure ìŒì„± ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
    } finally {
      setIsLoadingAudio(false);
    }
  };

  // ìŒì„± ì¬ìƒ
  const playAudio = async () => {
    if (!synthesizedAudioRef.current) {
      alert('ìŒì„±ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      return;
    }

    // ì´ë¯¸ ì¬ìƒ ì¤‘ì´ë©´ ì¤‘ì§€
    if (audioState.isPlaying) {
      stopAudio();
      return;
    }

    setAudioState(prev => ({ ...prev, isPlaying: true }));
    
    try {
      // ì˜¤ë””ì˜¤ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
      if (synthesizedAudioRef.current.readyState < 2) {
        await new Promise((resolve, reject) => {
          const timeout = setTimeout(() => {
            reject(new Error('ì˜¤ë””ì˜¤ ë¡œë“œ ì‹œê°„ ì´ˆê³¼'));
          }, 5000);
          
          synthesizedAudioRef.current!.addEventListener('canplaythrough', () => {
            clearTimeout(timeout);
            resolve(true);
          }, { once: true });
          
          synthesizedAudioRef.current!.addEventListener('error', () => {
            clearTimeout(timeout);
            reject(new Error('ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨'));
          }, { once: true });
        });
      }
      
      await synthesizedAudioRef.current.play();
      startTextHighlight();
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨:', error);
      setAudioState(prev => ({ ...prev, isPlaying: false }));
      
      if (error instanceof Error) {
        if (error.message.includes('ì‚¬ìš©ì')) {
          alert('ë¸Œë¼ìš°ì €ì—ì„œ ì˜¤ë””ì˜¤ ì¬ìƒì„ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
        } else {
          alert('ìŒì„± ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
        }
      } else {
        alert('ìŒì„± ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      }
    }
  };

  // ìŒì„± ì¤‘ì§€
  const stopAudio = () => {
    if (synthesizedAudioRef.current) {
      synthesizedAudioRef.current.pause();
      synthesizedAudioRef.current.currentTime = 0;
      // ì˜¤ë””ì˜¤ ì™„ì „ ì¤‘ì§€
      synthesizedAudioRef.current.load();
    }
    
    if (highlightTimeoutRef.current) {
      clearTimeout(highlightTimeoutRef.current);
      highlightTimeoutRef.current = null;
    }
    
    setAudioState(prev => ({ 
      ...prev, 
      isPlaying: false, 
      currentWordIndex: -1 
    }));
  };

  // í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŠ¸ ì• ë‹ˆë©”ì´ì…˜
  const startTextHighlight = () => {
    const words = text.replace(/[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]/g, ' ').split(/\s+/).filter(word => word.length > 0);
    let currentIndex = 0;

    const highlightNext = () => {
      if (currentIndex < words.length && audioState.isPlaying) {
        setAudioState(prev => ({ ...prev, currentWordIndex: currentIndex }));
        currentIndex++;
        
        // ê° ë‹¨ì–´ë‹¹ ì•½ 500ms ê°„ê²©ìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸ (ì‹¤ì œë¡œëŠ” ìŒì„± ê¸¸ì´ì— ë§ì¶° ì¡°ì •)
        highlightTimeoutRef.current = setTimeout(highlightNext, 500);
      }
    };

    highlightNext();
  };

  // ë…¹ìŒ ì‹œì‘
  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      // ì§€ì›ë˜ëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ í™•ì¸
      const supportedTypes = [
        'audio/wav',
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4'
      ];
      
      let selectedType = 'audio/webm;codecs=opus'; // ê¸°ë³¸ê°’
      
      for (const type of supportedTypes) {
        if (MediaRecorder.isTypeSupported(type)) {
          selectedType = type;
          break;
        }
      }
      
      console.log('ì„ íƒëœ ì˜¤ë””ì˜¤ í˜•ì‹:', selectedType);
      
      const options = {
        mimeType: selectedType,
        audioBitsPerSecond: 16000
      };
      
      mediaRecorderRef.current = new MediaRecorder(stream, options);
      audioChunksRef.current = [];

      mediaRecorderRef.current.ondataavailable = (event) => {
        audioChunksRef.current.push(event.data);
      };

      mediaRecorderRef.current.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: selectedType });
        setAudioState(prev => ({ 
          ...prev, 
          hasRecording: true, 
          audioBlob 
        }));
        
        // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorderRef.current.start();
      setAudioState(prev => ({ ...prev, isRecording: true }));
    } catch (error) {
      console.error('ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:', error);
      alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
    }
  };

  // ë…¹ìŒ ì¤‘ì§€
  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
      setAudioState(prev => ({ ...prev, isRecording: false }));
    }
  };

  // ë…¹ìŒ í† ê¸€
  const toggleRecording = () => {
    if (audioState.isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  };

  // ë°œìŒ í‰ê°€ ì‹œì‘
  const handleEvaluate = () => {
    if (audioState.audioBlob) {
      onEvaluate(audioState.audioBlob);
    }
  };

  return (
    <div className="space-y-8">
      <div className="text-center">
        <h2 className="text-3xl font-bold text-gray-800 mb-2">ì‰ë„ì‰ ì—°ìŠµ</h2>
        <p className="text-gray-600">ìŒì„±ì„ ë“£ê³  ë”°ë¼ ë§í•´ë³´ì„¸ìš”</p>
      </div>

             {/* ìŒì„± ì„ íƒ íŒì—… ì˜¤ë²„ë ˆì´ */}
       {showVoiceSelect && (
         <div className="fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center z-50">
           <div className="bg-white rounded-2xl p-6 w-full max-w-2xl mx-4">
             <div className="flex justify-between items-center mb-6">
               <h3 className="text-xl font-semibold text-gray-800">ìŒì„± ì„ íƒ</h3>
               <button
                 onClick={() => setShowVoiceSelect(false)}
                 className="text-gray-500 hover:text-gray-700 text-2xl"
               >
                 Ã—
               </button>
             </div>
             <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
               {VOICE_OPTIONS.map((voice) => (
                 <button
                   key={voice.id}
                   onClick={() => {
                     setSelectedVoice(voice);
                     setShowVoiceSelect(false);
                   }}
                   className={`p-4 rounded-lg border-2 transition-all duration-200 text-left ${
                     selectedVoice.id === voice.id
                       ? 'border-blue-500 bg-blue-50'
                       : 'border-gray-200 hover:border-gray-300 hover:bg-gray-50'
                   }`}
                 >
                   <div className="flex items-center justify-between mb-2">
                     <span className="font-medium text-gray-800">{voice.name}</span>
                     <div className={`w-3 h-3 rounded-full ${
                       voice.gender === 'male' ? 'bg-blue-500' : 'bg-pink-500'
                     }`}></div>
                   </div>
                   <p className="text-sm text-gray-600">{voice.description}</p>
                 </button>
               ))}
             </div>
           </div>
         </div>
       )}

      {/* ìŠ¤í¬ë¦½íŠ¸ í‘œì‹œ ì˜ì—­ */}
      <ScriptDisplay 
        text={text}
        currentWordIndex={audioState.currentWordIndex}
        isPlaying={audioState.isPlaying}
        audioElement={synthesizedAudioRef.current}
        onWordHighlight={(wordIndex) => {
          setAudioState(prev => ({ ...prev, currentWordIndex: wordIndex }));
        }}
      />

             {/* ì˜¤ë””ì˜¤ ì»¨íŠ¸ë¡¤ */}
       <AudioControls
         isPlaying={audioState.isPlaying}
         isRecording={audioState.isRecording}
         hasRecording={audioState.hasRecording}
         isLoadingAudio={isLoadingAudio}
         onPlay={playAudio}
         onStop={stopAudio}
         onToggleRecording={toggleRecording}
         onEvaluate={handleEvaluate}
         onToggleVoiceSelect={() => setShowVoiceSelect(!showVoiceSelect)}
         showVoiceSelect={showVoiceSelect}
       />

             {/* í•˜ë‹¨ ë„¤ë¹„ê²Œì´ì…˜ */}
       <div className="flex justify-between items-center pt-6">
         <button
           onClick={onGoBack}
           className="flex items-center space-x-2 px-6 py-3 bg-gray-500 text-white rounded-full hover:bg-gray-600 transition-all duration-300"
         >
           <span>â†</span>
           <span>ëŒì•„ê°€ê¸°</span>
         </button>

                   <div className="flex items-center space-x-4">
            {onFavorite && (
              <button
                onClick={onFavorite}
                className={`flex items-center space-x-2 px-6 py-3 rounded-full transition-all duration-300 ${
                  isFavorite 
                    ? 'bg-yellow-500 text-white hover:bg-yellow-600' 
                    : 'bg-yellow-400 text-white hover:bg-yellow-500'
                }`}
              >
                <span>{isFavorite ? 'â¤ï¸' : 'ğŸ¤'}</span>
                <span className="font-medium">
                  {isFavorite ? 'ì¦ê²¨ì°¾ê¸°ë¨' : 'ì¦ê²¨ì°¾ê¸°'}
                </span>
              </button>
            )}
           
           <div className="text-center">
             {audioState.isRecording && (
               <div className="flex items-center space-x-2 text-red-500">
                 <div className="w-3 h-3 bg-red-500 rounded-full animate-pulse"></div>
                 <span className="text-sm font-medium">ë…¹ìŒ ì¤‘...</span>
               </div>
             )}
             {audioState.hasRecording && !audioState.isRecording && (
               <div className="flex items-center space-x-2 text-green-500">
                 <span>âœ“</span>
                 <span className="text-sm font-medium">ë…¹ìŒ ì™„ë£Œ</span>
               </div>
             )}
           </div>
         </div>
       </div>
    </div>
  );
};

// ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë³„ë„ íŒŒì¼ë¡œ ë¶„ë¦¬)
const convertToPinyin = (text: string): string => {
  // ì‹¤ì œë¡œëŠ” ì¤‘êµ­ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ API ì‚¬ìš©
  const pinyinMap: { [key: string]: string } = {
    'ä½ ': 'nÇ', 'å¥½': 'hÇo', 'æˆ‘': 'wÇ’', 'æ˜¯': 'shÃ¬', 'æ¥': 'lÃ¡i', 'è‡ª': 'zÃ¬',
    'éŸ©': 'hÃ¡n', 'å›½': 'guÃ³', 'çš„': 'de', 'å­¦': 'xuÃ©', 'ç”Ÿ': 'shÄ“ng',
    'æ­£': 'zhÃ¨ng', 'åœ¨': 'zÃ i', 'ä¹ ': 'xÃ­', 'ä¸­': 'zhÅng', 'æ–‡': 'wÃ©n'
  };
  
  return text.split('').map(char => pinyinMap[char] || char).join(' ');
};

const generateAzureTTS = async (text: string, voiceId: string): Promise<string> => {
  // í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
  const subscriptionKey = import.meta.env.VITE_AZURE_SPEECH_KEY;
  const region = import.meta.env.VITE_AZURE_SPEECH_REGION || 'eastasia';

  console.log('ğŸ” í™˜ê²½ë³€ìˆ˜ í™•ì¸:', {
    subscriptionKey: subscriptionKey ? `âœ… ì„¤ì •ë¨ (${subscriptionKey.substring(0, 10)}...)` : 'âŒ ì„¤ì •ë˜ì§€ ì•ŠìŒ',
    region: region ? `âœ… ì„¤ì •ë¨ (${region})` : 'âŒ ì„¤ì •ë˜ì§€ ì•ŠìŒ'
  });

  if (!subscriptionKey) {
    throw new Error('VITE_AZURE_SPEECH_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
  }

  // XML ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
  const escapedText = text
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/"/g, '&quot;')
    .replace(/'/g, '&apos;');

  // ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
  const sentences = escapedText.split(/[ã€‚ï¼ï¼Ÿ.!?]/).filter(s => s.trim().length > 0);
  
  // ê° ë¬¸ì¥ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ SSML ìƒì„±
  const sentenceElements = sentences.map(sentence => 
    `<prosody rate="medium" pitch="medium">${sentence.trim()}</prosody>`
  ).join(' ');

  // ì„±ë³„ì— ë”°ë¥¸ gender ì†ì„± ì„¤ì •
  const gender = voiceId.includes('Xiaoxiao') || voiceId.includes('Xiaoyi') ? 'Female' : 'Male';
  
  const ssml = `<speak version='1.0' xml:lang='zh-CN'>
    <voice xml:lang='zh-CN' xml:gender='${gender}' name='${voiceId}'>
      ${sentenceElements}
    </voice>
  </speak>`;

  try {
    // API í‚¤ ê²€ì¦ ë° ì •ë¦¬
    const cleanKey = subscriptionKey?.trim().replace(/[^\x00-\x7F]/g, '');
    
    console.log('Azure TTS request started:', {
      region,
      textLength: text.length,
      keyLength: cleanKey?.length,
      keyPreview: cleanKey?.substring(0, 10) + '...'
    });

    const response = await fetch(`https://${region}.tts.speech.microsoft.com/cognitiveservices/v1`, {
      method: 'POST',
      headers: {
        'Ocp-Apim-Subscription-Key': cleanKey,
        'Content-Type': 'application/ssml+xml',
        'X-Microsoft-OutputFormat': 'audio-16khz-128kbitrate-mono-mp3'
      },
      body: ssml,
    });

    if (!response.ok) {
      const errorText = await response.text().catch(() => 'ì‘ë‹µì„ ì½ì„ ìˆ˜ ì—†ìŒ');
      
      if (response.status === 401) {
        throw new Error(`ì¸ì¦ ì‹¤íŒ¨: API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš” (${response.status})`);
      } else if (response.status === 403) {
        throw new Error(`ê¶Œí•œ ê±°ë¶€: êµ¬ë… ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš” (${response.status})`);
      } else {
        throw new Error(`TTS ìš”ì²­ ì‹¤íŒ¨: ${response.status} - ${errorText}`);
      }
    }

    const audioBlob = await response.blob();
    if (audioBlob.size === 0) {
      throw new Error('ë¹ˆ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.');
    }

    return URL.createObjectURL(audioBlob);
  } catch (error) {
    console.error('Azure TTS ì˜¤ë¥˜:', error);
    throw error;
  }
};

export default ShadowingPracticeStep; 